from typing import Optional

import torch
import torch.nn as nn
from einops import rearrange
from einops.layers.torch import Rearrange
import torch.nn.functional as F
from .vit import TransformerBlock

class Gating(nn.Module):
    def __init__(self, input_dim = 768, num_experts = 12, dropout_rate = 0.1):
        super(Gating, self).__init__()
        
        self.net = nn.Sequential(
            nn.LayerNorm(input_dim),
            nn.Linear(input_dim, 256),
            nn.GELU(),
            nn.Dropout(dropout_rate),
            nn.Linear(256, 128),
            nn.GELU(),
            nn.Dropout(dropout_rate),
            nn.Linear(128, num_experts)
        )

    def forward(self, x):
        """
        x: tensor of shape (batch_size, dim)
        returns: tensor of shape (batch_size, num_experts)
        """
        return torch.softmax(self.net(x), dim = 1)


class ST_MEM_ViT_MoL(nn.Module):
    def __init__(
        self,
        seq_len: int,
        patch_size: int,
        num_leads: int,
        num_classes: Optional[int] = None,
        width: int = 768,
        depth: int = 12,
        mlp_dim: int = 3072,
        heads: int = 12,
        dim_head: int = 64,
        qkv_bias: bool = True,
        drop_out_rate: float = 0.,
        attn_drop_out_rate: float = 0.,
        drop_path_rate: float = 0.,
    ):
        super().__init__()
        assert seq_len % patch_size == 0, 'The sequence length must be divisible by the patch size.'
        self._repr_dict = {
            'seq_len': seq_len,
            'patch_size': patch_size,
            'num_leads': num_leads,
            'num_classes': num_classes if num_classes is not None else 'None',
            'width': width,
            'depth': depth,
            'mlp_dim': mlp_dim,
            'heads': heads,
            'dim_head': dim_head,
            'qkv_bias': qkv_bias,
            'drop_out_rate': drop_out_rate,
            'attn_drop_out_rate': attn_drop_out_rate,
            'drop_path_rate': drop_path_rate,
        }

        self.width = width
        self.depth = depth
        self.avg = nn.AvgPool1d(depth, depth)

        # embedding layers
        num_patches = seq_len // patch_size
        patch_dim = patch_size
        self.to_patch_embedding = nn.Sequential(
            Rearrange('b c (n p) -> b c n p', p = patch_size),
            nn.LayerNorm(patch_dim),
            nn.Linear(patch_dim, width),
            nn.LayerNorm(width),
        )

        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 2, width))
        self.sep_embedding = nn.Parameter(torch.randn(width))
        self.lead_embeddings = nn.ParameterList(nn.Parameter(torch.randn(width)) for _ in range(num_leads))
        
        # transformer layers
        drop_path_rate_list = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]
        for i in range(depth):
            block = TransformerBlock(
                input_dim = width,
                output_dim = width,
                hidden_dim = mlp_dim,
                heads = heads,
                dim_head = dim_head,
                qkv_bias = qkv_bias,
                drop_out_rate = drop_out_rate,
                attn_drop_out_rate = attn_drop_out_rate,
                drop_path_rate = drop_path_rate_list[i],
            )
            self.add_module(f'block{i}', block)
        self.dropout = nn.Dropout(drop_out_rate)
        self.norm = nn.LayerNorm(width)
        self.gating = Gating(input_dim = width, num_experts = depth)
        
        # classifier head
        # self.head = nn.Identity() if num_classes is None else nn.Linear(width, num_classes)
        if num_classes is None:
            self.head = nn.Identity()
        else:
            layers_head = [
                nn.BatchNorm1d(self.width, eps = 1e-5, momentum = 0.1, affine = True, track_running_stats =  True),
                nn.Dropout(p = 0.25, inplace = False),
                nn.Linear(in_features = self.width, out_features = 256, bias = True),
                nn.ReLU(inplace = True),
                nn.BatchNorm1d(256, eps = 1e-5, momentum = 0.1, affine = True, track_running_stats =  True),
                nn.Dropout(p = 0.5, inplace = False),
                nn.Linear(in_features = 256, out_features = num_classes, bias = True)
            ]
            self.head = nn.Sequential(*layers_head)

    def reset_head(self, num_classes: Optional[int] = None):
        del self.head
        if num_classes is None:
            self.head = nn.Identity()
        else:
            layers_head = [
                nn.BatchNorm1d(self.width, eps = 1e-5, momentum = 0.1, affine = True, track_running_stats =  True),
                nn.Dropout(p = 0.25, inplace = False),
                nn.Linear(in_features = self.width, out_features = 256, bias = True),
                nn.ReLU(inplace = True),
                nn.BatchNorm1d(256, eps = 1e-5, momentum = 0.1, affine = True, track_running_stats =  True),
                nn.Dropout(p = 0.5, inplace = False),
                nn.Linear(in_features = 256, out_features = num_classes, bias = True)
            ]
            self.head = nn.Sequential(*layers_head)

    def forward_encoding(self, series):
        num_leads = series.shape[1]
        if num_leads > len(self.lead_embeddings):
            raise ValueError(f'Number of leads ({num_leads}) exceeds the number of lead embeddings')
        
        x = self.to_patch_embedding(series)
        b, _, n, _ = x.shape
        x = x + self.pos_embedding[:, 1:n + 1, :].unsqueeze(1)

        # lead indicating modules
        sep_embedding = self.sep_embedding[None, None, None, :]
        left_sep = sep_embedding.expand(b, num_leads, -1, -1) + self.pos_embedding[:, :1, :].unsqueeze(1)
        right_sep = sep_embedding.expand(b, num_leads, -1, -1) + self.pos_embedding[:, -1:, :].unsqueeze(1)
        x = torch.cat([left_sep, x, right_sep], dim = 2)
        lead_embeddings = torch.stack([lead_embedding for lead_embedding in self.lead_embeddings]).unsqueeze(0)
        lead_embeddings = lead_embeddings.unsqueeze(2).expand(b, -1, n + 2, -1)
        x = x + lead_embeddings
        x = rearrange(x, 'b c n p -> b (c n) p')
        x = self.dropout(x)
        x_transformer = x
        layerout = []
        
        for i in range(self.depth):
            x = getattr(self, f'block{i}')(x)
            # remove SEP embeddings
            x_rearrange = rearrange(x, 'b (c n) p -> b c n p', c = num_leads)
            x_wo_sep = x_rearrange[:, :, 1:-1, :]
            x_out = torch.mean(x_wo_sep, dim = (1, 2))
            layerout.append(x_out)

        x_transformer_rearrange = rearrange(x_transformer, 'b (c n) p -> b c n p', c = num_leads)
        x_transformer_rearrange = x_transformer_rearrange[:, :, 1:-1, :]
        x_transformer_out = torch.mean(x_transformer_rearrange, dim = (1, 2))
        
        return layerout, x_transformer_out

    def forward(self, series):
        layerout, x_transformer_out = self.forward_encoding(series)
        x_stack = torch.stack(layerout, dim = 2)
                     
        # Apply gating mechanism
        weights = self.gating(x_transformer_out)  # Shape: (batch_size, num_experts)
        x_fused = (x_stack.permute(0, 2, 1) * weights.unsqueeze(-1)).sum(dim = 1)

        return self.head(self.norm(x_fused))

    def __repr__(self):
        print_str = f"{self.__class__.__name__}(\n"
        for k, v in self._repr_dict.items():
            print_str += f'{k}={v},\n'
        print_str += ')'
        return print_str


def Adjust_Image(single_image):
    single_image = torch.transpose(single_image, 1,2)
    return single_image[0] # Just chan x leng, so 12 x 4k

def Adjust_Many_Images(image_batch):
    image_batch = torch.transpose(image_batch[:,0,:,:],1,2) # This model wants data N-Chan-Len
    return image_batch

def get_STMemVITMOL_process_single_image():
    return Adjust_Image

def get_STMemVITMOL_process_multi_image():
    return Adjust_Many_Images

def get_STMemVITMOL(num_classes, num_leads = 12, seq_len = 1000, patch_size = 50):
    model_args = dict(
        seq_len = seq_len,
        patch_size = patch_size,
        num_leads = num_leads,
        num_classes = num_classes,
        width = 768,
        depth = 12,
        heads = 6,
        mlp_dim = 3072,
        dim_head = 128
    )
    return ST_MEM_ViT_MoL(**model_args)